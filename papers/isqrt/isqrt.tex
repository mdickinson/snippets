\documentclass[a4paper]{article}

\usepackage{listings}
\usepackage{amsthm}
\usepackage{mathtools}

\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclareMathOperator{\length}{length}
\DeclareMathOperator{\rnd}{rnd}
\DeclareMathOperator{\fsqrt}{fsqrt}
\DeclareMathOperator{\fsqr}{fsqr}
\DeclareMathOperator{\float}{float}
\DeclareMathOperator{\rint}{rint}

\newcommand{\isqrt}[1]{\floor{\sqrt{#1}}}
\newcommand{\csqrt}[1]{\ceil{\sqrt{#1}}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\title{Python's integer square root algorithm}
\author{Mark Dickinson}

\begin{document}
\lstset{language=Python}
\maketitle
\begin{abstract}
We present a novel adaptive-precision variant of Heron's method for computing
integer square roots of arbitrary-precision integers. The algorithm is unusual
in that it is almost branch-free, requiring only a single final correction
step, and in that the number of required iterations is easily predicted in
advance. The algorithm is efficient both at small scales and asymptotically,
and represents an attractive compromise between speed and simplicity. Since
Python 3.8, the algorithm is used by the CPython implementation of the Python
programming language for its standard library integer square root function.
\end{abstract}
\section{Introduction}

For a nonnegative integer~$n$, the \emph{integer square root} of~$n$ is the
unique nonnegative integer~$a$ satifying $a^2 \le n < (a + 1)^2$. Equivalently,
it's the integer part of the nonnegative real square root of~$n$, or
$\floor{\sqrt n}$. The integer square root is a basic building block of any
arbitrary-precision arithmetic toolkit: many number-theoretic and cryptographic
algorithms require either the ability to find integer square roots, or the
slightly weaker ability to detect whether a given integer is a square, and if
so to extract its square root.

The Python programming language has an arbitrary-precision integer type
\lstinline$int$, and since Python 3.8 the Python standard library implements an
integer square root operation, \lstinline$math.isqrt$. The implementation of
this operation in the reference CPython implementation of the Python language
was subject to competing concerns. The operation should be fast for small
inputs (overwhelmingly the most common use-case) while remaining reasonably
efficient even for large inputs. However, the CPython maintainers also value
portability and maintainability of the codebase, which argues for not using a
complex algorithm where a simpler one would be good enough. This paper
describes the algorithm that was used in CPython, which (in the author's
opinion) hits the sweet spot between the various concerns, giving an elegant
compromise between efficiency and simplicity.

Before introducing the new algorithm, we review a common integer-based
approach, Heron's method, in section~\ref{old_method}. This forms the basis of
the algorithm currently used for Java's \lstinline{BigInteger.isqrt} method,
for example. Section~\ref{old_method} provides much of the background we need
to introduce our algorithm, which is presented in section~\ref{new_method}.

\section{Heron's method}
\label{old_method}

In this section we describe a well-known approach to computing integer square
roots, based on an adaptation of Heron's method to the domain of positive
integers.

Heron's method, also known as the Babylonian method, and perhaps more
recognisable in recent centuries as a special case of the Newton-Raphson
method, is a procedure for approximating the square root of a positive
real number~$t$. The idea is that if $x$ is a positive real
approximation to~$\sqrt t$, then~$h(x)$, defined by
$$h(x) = \frac{x + t/x}2,$$ is an improved approximation. Applying~$h$ to that
improved approximation yields a still better approximation~$h(h(x))$, and so
on. It can be shown that the sequence of iterates of~$h$ on~$x$ converges
towards~$\sqrt t$ from any positive initial value~$x$, and that once the
sequence gets sufficiently close to~$\sqrt t$, convergence is quadratic, so
that the number of correct decimal places roughly doubles with each iteration.

\begin{example}
  Suppose $t=2$. Given an approximation $7/5 = 1.4$ to the square root
  $1.414213562\dots$ of $t$, a single iteration of Heron's method produces a
  new approximation, accurate to $4$ decimal places after the point:
  $$h(7/5) = \frac{7/5 + 2/(7/5)}2 = 99/70 = 1.414285714\dots.$$ Applying a
  second iteration with $99/70$ as input gives
  $$h(h(7/5)) = h(99/70) = 19601/13860 =
  1.414213564\dots,$$ which is accurate to $8$ decimal places. A third
  iteration gives a value accurate to $17$ decimal places.
\end{example}

Heron's method can be adapted to the domain of positive integers. For the
remainder of this section we assume that $n$ is a fixed positive integer, and
we define a function $g$ on the positive integers by
$$g(a) = \floor*{\frac{a + \floor{n/a}}2}.$$

The hope is that, just like its real counterpart~$h$, the function $g$ will
transform a poor approximation to the integer square root of $n$ into an
improved approximation, and so by applying $g$ repeatedly we eventually reach
the integer square root of~$n$. This works, but we have to be a little careful
with the details, and in particular the termination condition. The lemmas below
make this precise.

\begin{lemma}\label{heron_high}
  For any positive integer $a$, $\floor{\sqrt n} \le g(a)$.
\end{lemma}

\begin{proof}
  Expanding and rearranging the inequality $0 \le (a - \sqrt n)^2$ gives
  $$2a\sqrt n \le a^2 + n.$$
  Dividing through by $2a$ gives
  $$\sqrt n \le \frac{a + n/a}2,$$
  which can also be seen as the AM-GM inequality applied to~$a$ and~$n/a$.
  Hence
  $$\floor{\sqrt n} \le \floor*{\frac{a + n/a}2}
  = \floor*{\frac{\floor{a + n/a}}2}
  = \floor*{\frac{a + \floor{n/a}}2} = g(a).$$
\end{proof}

\begin{lemma}\label{heron_decreases}
  Suppose~$a$ is a positive integer satisfying~$\floor{\sqrt n} < a$. Then
  $g(a) < a$.
\end{lemma}

\begin{proof}
  We have the following chain of equivalences:
  \begin{align*}
    \floor{\sqrt n} < a &\iff \sqrt n < a \\
                        &\iff n < a^2 \\
                        &\iff n/a < a \\
                        &\iff \floor{n/a} < a \\
                        &\iff a + \floor{n/a} < 2a \\
                        &\iff \frac{a + \floor{n/a}}2 < a \\
                        &\iff \floor*{\frac{a + \floor{n/a}}2} < a \\
                        &\iff g(a) < a.
  \end{align*}
  This completes the proof.
\end{proof}

Now let~$a$ be any integer satisfying $\floor{\sqrt n} \le a$, and let
$$a_0, a_1, a_2, \dots$$ be the sequence of iterates of $g$ applied to $a$;
that is, $a_0 = a$ and for all~$i \ge 0$, $a_{i+1} = g(a_i)$. We'll show that
this sequence must eventially reach~$\floor{\sqrt n}$.

\begin{lemma}\label{sequence_high}
  For all $i\ge 0$, $\floor{\sqrt n} \le a_i$.
\end{lemma}
\begin{proof}
  This follows directly from Lemma~\ref{heron_high} for $i\ge 1$, and
  from our assumption on~$a$ for $i=0$.
\end{proof}

\begin{lemma}\label{isqrt_condition}
  For all $i\ge 0$, $\floor{\sqrt n} = a_i$ if and only if $a_i\le a_{i+1}$.
\end{lemma}

\begin{proof}
  By Lemma~\ref{sequence_high}, either $\floor{\sqrt n} < a_i$ or $\floor{\sqrt
  n} = a_i$. If $\floor{\sqrt n} = a_i$ then by Lemma~\ref{sequence_high}
  again, $a_i \le a_{i+1}$. If $\floor{\sqrt n} < a_i$ then $a_{i+1} < a_i$ by
  Lemma~\ref{heron_decreases}.
\end{proof}

\begin{theorem}
  There exists an~$i\ge 0$ such that $a_i$ is the integer square root of~$n$.
\end{theorem}

\begin{proof}
  For an fixed positive number $n$, there exists a subset $S=\lbrace a_i \rbrace$ of natural number
  where each element $a_i$ is obtained by the iteration $a_{i+1}=g(a_i)$ and $a_0=a$. 
  Hence $S$ is non-empty set. By the well-ordering principle, there's an~$i$ for which $a_i$ is minimal
  amongst all elements of the sequence. That minimality implies $a_i \le
  a_{i+1}$, and so by Lemma~\ref{isqrt_condition}, $a_i$ is the integer square
  root of~$n$.
\end{proof}

Note that while the sequence is guaranteed to encounter $\floor{\sqrt n}$
eventually, it's \emph{not} necessarily true that the sequence \emph{converges}
to~$\floor{\sqrt n}$, in the sense that it's eventually constant. For
example, if $n=15$, the sequence of iterates of $g$ eventually alternates
between $3$ and $4$, and more generally a similar alternation will occur for
any $n$ of the form $a^2 - 1$ for some $a \ge 2$.

By the results above, we can find $\floor{\sqrt n}$ by taking any starting
point $a \ge \floor{\sqrt n}$, and replacing $a$ with $g(a)$ until $a\le g(a)$.
Listing~\ref{isqrt_heron_direct} expresses this algorithm in Python, using
$a=n$ as the starting point for the iteration. Note that the function
\lstinline$isqrt$ assumes $n > 0$. Extending the code to return~$0$ for $n=0$
and to raise an exception for negative~$n$ is left as an easy exercise for the
reader.

A linguistic note for readers unfamiliar with Python: the \lstinline$//$
operator performs ``floor division'': \lstinline$n//a$ represents $\floor{n /
a}$. Indeed, from a computational perspective, an expression like $\floor{n /
a}$ is misleading: it resembles a composition of two operations, while most
programming languages or arbitrary-precision integer-arithmetic packages will
provide some form of integer division as an integer-to-integer primitive.

\lstinputlisting[
  label=isqrt_heron_direct,
  float,
  frame=single,
  caption={Integer square root via Heron's method, version 1}]
  {isqrt_heron_direct.py}

The use of~$n$ as an initial estimate is inefficient for large~$n$: the initial
iterations will roughly halve the estimate each time, and many iterations will
be needed to get into the general neighborhood of the square root. It would be
better to choose a starting value~$a$ that's closer to $\sqrt n$. If we can't
guarantee that $a\ge \floor{\sqrt n}$, we can replace $a$ with $g(a)$ before
running the main algorithm: by Lemma~\ref{heron_high}, $g(a)$ is guaranteed to
satisfy $g(a) \ge \floor{\sqrt n}$.

One simple and reasonably efficient possibility for the starting value is to
use the smallest power of two exceeding $\floor{\sqrt n}$. Before giving the
code, we make a definition.

\begin{definition}
  For a nonnegative integer $n$, the \emph{bit length} of $n$, which we'll
  write simply as $\length(n)$, is the least nonnegative integer $e$ for which
  $n < 2^e$.
\end{definition}

For positive $n$, the bit length of $n$ is $1 + \floor{\log_2(n)}$, while the
bit length of zero is zero. In Python, the bit length of a nonnegative integer
\lstinline$n$ can be computed as \lstinline$n.bit_length()$.

Given nonnegative integers $n$ and $e$, we have the following chain
of equivalences:
\begin{align*}
  \floor{\sqrt n} < 2^e
    &\iff \sqrt n < 2^e \\
    &\iff n < 2^{2e} \\
    &\iff \length(n) \le 2e \\
    &\iff \length(n) < 2e + 1 \\
    &\iff \floor{(\length(n) - 1) / 2} < e \\
    &\iff \floor{(\length(n) + 1) / 2} \le e
\end{align*}
So taking $e = \floor{(\length(n) + 1) / 2}$, $2^e$ is the smallest power of
two that strictly exceeds the square root of~$n$. Using that as our starting
guess, and taking the opportunity to streamline the previous code a little at
the same time, we get the algorithm in Listing~\ref{isqrt_heron_improved}, in
which the termination condition $a \le g(a)$ has been replaced with the
equivalent condition $a \le \floor{n/a}$.

\lstinputlisting[
  label=isqrt_heron_improved,
  float,
  frame=single,
  caption={Integer square root via Heron's method, streamlined}]
  {isqrt_heron_improved.py}

We chose $e$ to satisfy $\floor{\sqrt n} < 2^e$. For correctness, we only need
to satisfy the weaker condition $\floor{\sqrt n} \le 2^e$. Given that, we could
tighten our initial bound slightly by using $\length(n-1)$ in place of
$\length(n)$. But that costs an extra operation for all inputs $n$, for a
benefit only in the rare case that $n$ is an exact power of four. As such, the
change is probably not valuable.

Another simple and easily-computed starting guess is given by $a = \floor{n /
2^e}$ where $e =\floor{(\length(n) - 1) / 2}$. We leave it to the reader to
verify that this value for~$a$ also satisfies $\floor{\sqrt n} \le a$. There's
no obvious reason to prefer either form of starting guess over the other: in
particular, neither value is consistently smaller than the other for all~$n$.

The algorithm shown in Listing~\ref{isqrt_heron_improved} is well-known and
well-used. It has the virtues of simplicity and easy-to-establish correctness,
and is reasonably efficient for inputs that aren't too large. Nevertheless,
there are at least three areas in which there's room for improvement. First,
for large inputs (say a few thousand bits or more), the first few iterations of
the algorithm are performing expensive full-precision divisions to obtain only
a handful of new correct bits at each iteration. Second, our particular choice
of initial guess is somewhat ad hoc, and could probably be improved, possibly
at the expense of some additional complexity: in that sense, the algorithm
feels incomplete, or at least unsatisfyingly inelegant---it's really the
combination of two separate algorithms: one to find an initial guess, and a
second to improve that guess until we hit the integer square root. Finally,
there's potential inefficiency towards the end of the algorithm, and in
particular the fact that the termination condition requires that we perform an
extra division \emph{after} we've reached the desired result seems like an
unnecessary inefficiency. To illustrate the last point, consider the following
example.

\begin{example}
  Take $n = 16785408$, which is just a little larger than $2^{24} = 16777216$.
  Our starting guess for the square root is $2^{13} = 8192$. Successive
  iterations give $5120$, $4199$, $4098$, $4097$, and finally $4096$, which
  is the integer square root. Each iteration requires one division, and
  a final division is needed to establish the termination condition, for
  a total of six divisions.
\end{example}

So even starting from $4098$, just a distance of two away from the true integer
square root, three more divisions are required before the correct integer
square root can be identified and returned. Note that this example is not
typical: it was deliberately chosen to show worst-case behaviour.

The algorithm introduced in the following section addresses all three of these
deficiencies, at the expense of only a little extra complexity.

\section{Variable-precision Heron's method}
\label{new_method}

In this section we introduce a simple variant of Heron's method that's
more efficient than the basic algorithm for large inputs. As in
the previous section, we assume that~$n$ is a positive integer, and our aim
is to compute the integer square root of~$n$.

There are two key ideas. First, we vary the working precision as we go: our
algorithm produces at each iteration an integer approximation to the square
root of $\floor{n/4^f}$ for some integer $f$, with $f$ decreasing to $0$ as the
iterations progress. Second, we don't insist on obtaining the exact integer
square root of $\floor{n/4^f}$ at each iteration (which would require a
per-iteration check-and-correct step), but instead allow the error to
propagate, and we show that with careful control of the rate at which the
precision increases, the error remains bounded throughout the algorithm. We
then use a single check-and-correct step at the end of the algorithm.

To describe the algorithm, it's convenient to introduce a new notion: that of a
``near square root".

\begin{definition}
  Suppose $n$ is a positive integer. Call a positive integer $a$ a
  \emph{near square root} of $n$ if $(a - 1)^2 < n < (a + 1)^2$.
\end{definition}

In other words, $a$ is a near square root of $n$ if $a$ is either $\floor{\sqrt
n}$ or $\ceil{\sqrt n}$. In particular, if $n = a^2$ is a perfect square then
$a$ is the \emph{only} near square root of $n$; otherwise, $n$ has two near
square roots.

Given a near square root $a$ of $n$, the integer square root of $n$ is clearly
either $a$ or $a-1$, depending on whether $a^2 \le n$ or $a^2 > n$
(respectively). So an algorithm for computing near square roots provides us
with a way to compute integer square roots, and in the remainder of this
section we focus on finding near square roots.

Our algorithm is based on the idea of ``lifting" a near square root of a
quotient of $n$ to a near square root of $n$. Suppose that $j$ is a positive
integer satisfying $j^2 \le n$ and $b$ is a near square root of $\floor{n /
j^2}$. Then $b$ is an approximation to $\sqrt n / j$, and so $jb$ is an
approximation to $\sqrt n$. A single iteration of the integer form of Heron's
method applied to $jb$ then gives an improved approximation
$$\floor*{\frac{jb + \floor{n / jb}}2} \sim \sqrt n.$$ If $j$ is not too large
relative to~$n$, this improved approximation will again be a near square root
of~$n$.

Here's a theorem that makes that ``not too large" bound precise. For
simplicity, we restrict~$j$ to be an even integer: write $j = 2k$, then in the
above discussion, $b$ is a near square root of~$\floor{n / 4k^2}$, $2kb$ is an
approximation to $\sqrt n$, and our improved approximation is $kb + \floor{n /
4kb}$.

\begin{theorem}\label{main_theorem}
  Suppose that $n$ and $k$ are positive integers satisfying $4k^4 \le n$. Let
  $b$ be a near square root of $\floor{n / 4k^2}$. Then the positive integer
  $a$ defined by
  $$a = kb + \floor*{\frac{n}{4kb}}$$
  is a near square root of $n$.
\end{theorem}

\begin{proof}
  By definition of near square root, we have
  \begin{equation}\label{b_nsqrt}
    (b - 1)^2 < \floor*{\frac{n}{4k^2}} < (b + 1)^2.
  \end{equation}
  Since $(b + 1)^2$ is an integer, we can remove the floor brackets to obtain
  \begin{equation}\label{b_nsqrt_weak}
    (b - 1)^2 < \frac{n}{4k^2} < (b + 1)^2.
  \end{equation}
  Multiplying by $4k^2$ throughout \eqref{b_nsqrt_weak} and then taking square
  roots gives
  \begin{equation}
    2k(b - 1) < \sqrt n < 2k(b + 1)
  \end{equation}
  which can be rearranged to the equivalent statement
  \begin{equation}
    \abs{2kb - \sqrt n} < 2k.
  \end{equation}
  Squaring and then dividing through by $4kb$ gives
  \begin{equation}
    0 \le kb + \frac{n}{4kb} - \sqrt n < k/b,
  \end{equation}
  which implies that
  \begin{equation}
    -1 < kb + \floor*{\frac{n}{4kb}} - \sqrt n < k/b.
  \end{equation}
  Substituting the definition of $a$ gives
  \begin{equation}\label{a_close_to_sqrt_n}
    -1 < a - \sqrt n < k/b.
  \end{equation}
  To complete the proof, we need to know that $k / b \le 1$. From our (so far
  unused) assumption that $4k^4 \le n$ we have $k^2 \le n / 4k^2$, while from
  the right-hand side of \eqref{b_nsqrt_weak} we have $n / 4k^2 < (b + 1)^2$.
  Combining these and taking square roots, $k < b + 1$, hence $k \le b $ and $k
  / b \le 1$. So now combining this with \eqref{a_close_to_sqrt_n} gives
  \begin{equation}
    -1 < a - \sqrt n < 1
  \end{equation}
  from which $(a - 1)^2 < n < (a + 1)^2$, so $a$ is a near square of~$n$, as
  required.
\end{proof}

\begin{example}
  Let $n = 46696$ and $k=10$. Then $\floor{n / k^2} = 116$, so $10$ and
  $11$ are both near square roots of $\floor{n / k^2}$.

  Taking $b = 10$, we get $a = 100 + \floor{46696 / 400} = 216$. Since
  $\sqrt{46696} = 216.092572755\dots$, $216$ is indeed a near square root of
  $n$. If we take $b = 11$, we also get $a = 110 + \floor{46696 / 440} = 216$.
\end{example}

Now we turn to implementation. Like many arbitrary-precision integer
implementations, Python's integer implementation is binary-based, so
multiplications and floor divisions by powers of two can be performed
efficiently by bit-shifting. So in applying Theorem~\ref{main_theorem}, we'd
like to choose $k$ to be a power of two satisfying $4k^4 \le n$, and in order
for the algorithm to proceed as fast as possible we'd like $k$ to be the
largest such power. Writing $k=2^e$, we have:
\begin{align*}
  4k^4 \le n
  &\iff 2^{2 + 4e} \le n \\
  &\iff 2 + 4e < \length n \\
  &\iff 3 + 4e \le \length n \\
  &\iff e \le \floor*{\frac{\length n - 3}{4}}
\end{align*}
So we take $k = 2^e$, where $e={\floor{(\length n - 3)/4}}$. This gives the
recursive near square root implementation shown in
Listing~\ref{isqrt_recursive}, where the multiplication by $k$ and the
divisions by $4k$ and $4k^2$ are replaced by the corresponding bit-shift
operations.

\lstinputlisting[
  label=isqrt_recursive,
  float,
  frame=single,
  caption={Adaptive-precision Heron's method, recursive}]
  {isqrt_recursive.py}

Each step of the algorithm shown in Listing~\ref{isqrt_recursive} involves
three big-integer shifts, one big-integer addition, one bit-length computation,
and one big-integer division, along with a handful of operations that only
involve small integers. We can improve this slightly: one of the two
right-shifts can be eliminated, by keeping track of the amount by which~$n$
should be shifted in the recursive call instead of actually shifting. With a
little more bookkeeping, we can also replace the per-iteration bit-length
computation with a single initial bit-length computation. These changes
represent minor efficiency improvements at the expense of some small loss of
clarity; we leave it to the interested reader to pursue them further.

The \lstinline{math.isqrt} implementation in CPython 3.8 doesn't use the
recursive version shown in Listing~\ref{isqrt_recursive}. Instead, we unwind
the recursion to obtain an iterative version of the algorithm. This version is
presented in Listing~\ref{isqrt_iterative}. It may not be immediately obvious
that Listing~\ref{isqrt_iterative} is equivalent to
Listing~\ref{isqrt_recursive}. Rather than describing the equivalence and
establishing the correctness of the iterative version via that equivalence,
we give a direct proof of correctness for the iterative version.

\lstinputlisting[
  label=isqrt_iterative,
  float,
  frame=single,
  caption={Adaptive-precision Heron's method, iterative}]
  {isqrt_iterative.py}


We establish two loop invariants on the variables $s$, $d$ and $a$. These
invariants hold just before entering the \lstinline$while$ loop and at the end
of every iteration of that loop, and hence also the beginning of any subsequent
iteration. The first loop invariant is $d = \floor{c/2^s}$; this should be
clear from examining the code. The second loop invariant states that at every
step, $a$ is a near square root of $\floor{n / 4^{c-d}}$. In particular, on
exit from the \lstinline$while$ loop, $s = 0$, $d = \floor{c/2^0} = c$ and so
$a$ is a near square root of $\floor{n / 4^{c-c}} = n$.

To establish the second invariant, note first that the invariant holds on the
first entry to the \lstinline$while$ loop: we have $c = \floor{\log_4 n}$, so
$4^c \le n < 4^{c+1}$, $1 \le \floor{n / 4^c} < 4$, and $a = 1$ is a near
square root of $\floor{n / 4^c}$. We then need to establish that if the
invariant holds at the beginning of any \lstinline$while$ loop iteration, it
also applies at the end of that iteration. We prove this via the following
lemma, in which $b$ and $e$ represent the values of $a$ and $d$ at the start of
the iteration.

\begin{lemma}
  Suppose that $0 \le s < \length(c)$, that $e = \floor{c / 2^{s + 1}}$, that
  $d = \floor{c / 2^s}$, and that $b$ is a near square root of $\floor{n /
  4^{c-e}}$. Let
  $$a = 2^{d-e-1}b + \floor*{\frac{n}{2^{2c-d-e+1}b}}.$$ Then $a$ is a near
  square root of $\floor{n / 4^{c-d}}$.
\end{lemma}

\begin{proof}
  Let $m = \floor{n / 4^{c-d}}$. Then $b$ is a near square root of
  $\floor{m/4^{d-e}}$ and we can rewrite $a$ as
  $$a = 2^{d-e-1}b + \floor*{\frac{m}{4\cdot 2^{d-e-1}b}}.$$ Now we can apply
  the main theorem: if we can show that $2^{d-e-1}$ is an integer and that
  $4(2^{d-e-1})^4 \le m$, it follows from Theorem~\ref{main_theorem} that $a$
  is a near square root of $m$. But from the definitions of $d$ and $e$, $1 \le
  d$ and $e = \floor{d/2}$, so it follows that $0 \le d - e - 1 \le e$, and
  \begin{align*}
    4(2^{d-e-1})^4 &= 4(4^{d-e-1})^2 \\
                  &= 4\cdot 4^{d-e-1}\cdot 4^{d-e-1} \\
                  &\le 4\cdot 4^{d-e-1}\cdot 4^e \\
                  &= 4^d
  \end{align*}
  Furthermore, since $c = \floor{\log_4 n}$, $4^c \le n$, so $4^d \le n /
  4^{c-d}$, hence $4^d \le m$. Combining this with the inequality above,
  $4(2^{d-e-1})^4 \le m$, as required.
\end{proof}

The quantities $c$, $d$, $e$, and $s$ are all small (machine-size) integers;
only $a$ and $n$ are big integers. So the algorithm consists of two big-integer
shifts, one big-integer addition and one big-integer division per iteration,
along with a handful of operations with small integers.

The total number of iterations $m$ of the while loop is remarkably simple: it's
exactly $\floor*{\log_2 \floor{\log_2 n}}$, assuming $n\ge 2$ (and zero
iterations are required for $n = 1$). So for example, input values $n$
satisfying $2^{32} \le n < 2^{64}$ require exactly five iterations, while
values in the range $2^{64} \le n < 2^{128}$ require exactly six.

\section{Near square roots and square detection}

Occasionally all that's required is the ability to detect whether a given
integer~$n$ is a perfect square. Typical efficient square-detection algorithms
perform a series of cheap tests on the value~$n$ to eliminate classes of
non-square integers - for example, by looking at the residue of~$n$
modulo~$256$ or~$255$. If~$n$ passes those tests, the final step is to compute
the integer square root~$a$ of~$n$ and check whether~$a^2 = n$.
For that final step it's sufficient to compute a near square root rather than
an integer square root: given an algorithm \lstinline{nsqrt} that computes near
square roots of positive integers, a positive integer~$n$ is a square if and
only if $n$ is equal to the square of~\lstinline$nsqrt(n)$. This means that for
the purposes of square detection, we can discard the final correction step from
the algorithm shown in Listing~\ref{isqrt_recursive}. This is shown in
Listing~\ref{is_square_recursive}.

\lstinputlisting[
  label=is_square_recursive,
  float,
  frame=single,
  caption={Perfect square detection using nsqrt}]
  {is_square_recursive.py}

Similarly, if we somehow already know that an integer~$n$ is a perfect square
and we want to compute its square root, we can omit the correction step in
Listing~\ref{isqrt_recursive}: the \lstinline$nsqrt$ function will compute the
square root directly.

\section{Fixed-precision algorithms}

The iterative algorithm shown in Listing~\ref{isqrt_iterative} specialises
easily to particular fixed-precision cases, giving an almost branch-free
algorithm. For example, if $2^{30} \le n < 2^{32}$, then $c = 15$, $s=4$, and
we can compute in advance the set of $d$ and $e$ values for each of the four
iterations. For smaller positive~$n$, we can find an $f$ such that $2^{30} \le
4^f n < 2^{32}$, apply the same algorithm to $4^f n$, then shift the resulting
near square root right by~$f$ bits. This gives the algorithm shown in
Listing~\ref{isqrt_32bit}. The corresponding 64-bit version is shown in
Listing~\ref{isqrt_64bit}. While the original iterative algorithm was developed for
positive~$n$, these fixed-precision variants also turn out to give the correct
answer in the case~$n = 0$.

\lstinputlisting[
  label=isqrt_32bit,
  float,
  frame=single,
  caption={Fixed-precision variant, valid for $0 \le n < 2^{32}$}]
  {isqrt_32bit.py}

\lstinputlisting[
  label=isqrt_64bit,
  float,
  frame=single,
  caption={Fixed-precision variant, valid for $0 \le n < 2^{64}$}]
  {isqrt_64bit.py}

For practical implementation, we can replace the initial steps of the algorithm
with a lookup table. Listing~\ref{isqrt_32bit_c} gives an implementation of a
32-bit integer square root in standard C. It makes use of a lookup table, and
requires just a single division. As written, that division is of a 32-bit
dividend by a 32-bit divisor, but both the divisor and the quotient fit into a
16-bit unsigned integer. The code also assumes the existence of a 32-bit "count
leading zeros" function, \lstinline$clz32$.

\lstinputlisting[
  label=isqrt_32bit_c,
  float,
  frame=single,
  caption={32-bit integer square root in C}]
  {isqrt32.c}

The rough shape of the code is straightforward: we first deal with
the special case where $x$ is zero. Then we normalise $x$, shifting left
by an even number of bits to ensure that $2^{30} \le x < 2^{32}$. Then
we compute the integer square root of our new $x$, and finally shift back
right to compensate for the original shift left. In the center of the function,
for $2^{30} \le x < 2^{32}$, the line with the table lookup provides a near
square root for $\floor{x / 2^{16}}$. The following line lifts that to a near
square root $y$ for~$x$, and then the line after that corrects $y$ downwards if
necessary.

There are a couple of subtleties to note. Immediately before the correction
step, from the bounds on~$x$ and the fact that~$y$ is a near square root
of~$x$, we know that $2^{15} \le y \le 2^{16}$. If $y = 2^{16}$ then it
won't fit into a 16-bit unsigned integer, so we need to exclude this
possibility. But if $y = 2^{16}$, then since $y$ is a near square root of $x$,
we must have $x > (2^{16} - 1)^2$. It then follows that in the previous line we
end up using the very last entry in the lookup table to yield $y = 256$, and
from there that after the lifting $y = 2^{16} - 1$, a contradiction. So $y =
2^{16}$ is impossible at this stage, and $y$ will always fit into a 16-bit
unsigned integer.

To make the lookup table work, the key fact is that given a 16-bit integer $t$
satisfying $2^{14} \le t < 2^{16}$, we can compute a near square root of $t$
using only the most significant 8 bits of~$t$. More generally, we have the
following lemma, which shows that we also have a small amount of freedom in
choosing the lookup table entries to be used.

\begin{lemma} Suppose that $n$ and $k$ are positive integers satisfying
  $(k+1)^2 \le 4n$, and let $m = \floor{n / k}$.
  Then $a \coloneqq \csqrt{2k(m+1)} - 1$ and $b \coloneqq \csqrt{2km}$
  are both near square roots of~$n$.
\end{lemma}

\begin{proof}
  We have:
    $$\csqrt{2km} - 1
      < \sqrt{2km}
      \le \sqrt n
      < \sqrt{2k(m+1)}
      \le \csqrt{2k(m+1)}.$$
  Or in other words,
    $$b - 1 < \sqrt n < a + 1.$$
  So
    $$(b - 1)^2 < n < (a + 1)^2.$$
  All that remains is to show that $a \le b$, and then we have
    $$(a - 1)^2 \le (b - 1)^2 < n < (a + 1)^2 \le (b + 1)^2,$$
  showing that both $a$ and $b$ are near square roots of $n$.

  But from our assumption that $k^2 \le n$ we have $k/2 < n / 2k$

\end{proof}

Now put $k=128$ in the above to get:

\begin{lemma} Suppose that $2^{14} \le t < 2^{16}$, and let $s = \floor{t /
256}$. Then $a \coloneqq \csqrt{256(s+1)} - 1$ and $b \coloneqq \csqrt{256s}$
are both near square roots of~$t$.
\end{lemma}

\begin{proof}
We have $\sqrt{256s} \le \sqrt{t} <
\sqrt{256(s+1)}$. So
$$\csqrt{256s} - 1 < \sqrt{256s} \le \sqrt t < \sqrt{256(s+1)} \le \csqrt{256(s+1)}.$$
Or in other words,
$$b - 1 < \sqrt t < a + 1,$$
so
$$(b - 1)^2 < t < (a + 1)^2.$$
All that remains is to show that~$a\le b$, and then we have
$$(a - 1)^2 \le (b - 1)^2 < t < (a + 1)^2 \le (b + 1)^2,$$ showing that
both~$a$ and~$b$ are near square roots of~$t$. But from our assumption that
$2^{14} \le t$, we have~$64 \le s$, and it follows that $128 \le \sqrt{256s}
\le \sqrt{256(s+1)}$ and hence
$$256 \le \sqrt{256(s+1)} + \sqrt{256s}$$ Multiplying both sides
by~$(\sqrt{256(s+1)} - \sqrt{256s}) / 256$ gives:
$$\sqrt{256(s+1)} - \sqrt{256s} \le 1.$$ Now rearranging and taking ceilings
gives
$$a = \csqrt{256(s+1)} - 1 \le \csqrt{256s} = b$$ as desired.
\end{proof}

In~Listing~\ref{isqrt_32bit_c}, the lookup table value $T(k)$ at position~$k$
is given by $T(k) = \isqrt{256(k + 64) - 1}$. So by the above lemma applied
to~$t=\floor{n / 2^{16}}$,
$$1 + T(\floor{n / 2^{24}} - 64) = \csqrt{256 \floor{n / 2^{24}}}$$ gives a
near square root of~$\floor{n / 2^{16}}$. It would have been more natural to
absorb the addition of $1$ into the table values, but that would have meant
that the final table value would no longer fit in an 8-bit integer, which in
turn would require the whole table to use 16-bit integers, wasting significant
space.

Finally, a 64-bit integer square root algorithm is shown
in~Listing~\ref{isqrt_64bit_c}. The same subtleties apply as with the 32-bit
integer square root: for values of~$x$ larger than $(2^{32} - 1)^2$, both~$255$
and~$256$ are near square roots of~$\floor{x/2^{56}}$. However, a result
of~$256$ from the table lookup would lead to~$y$ overflowing a 32-bit
unsigned integer, while with a value of~$255$, $y$ remains representable
throughout the algorithm. This is the opposite situation to the 32-bit
case, where we needed the table lookup to produce~$256$ rather than $255$
for~$x$ near the upper limits of the input range.

\lstinputlisting[
  label=isqrt_64bit_c,
  float,
  frame=single,
  caption={64-bit integer square root in C}]
  {isqrt64.c}

\section{Using floating-point}

Floating-point methods can be used either to compute integer square roots
directly for small~$n$, or to accelerate the computation of an integer square
root for larger~$n$ by providing an accurate starting guess for an iterative
algorithm. In this section we give some details. Note however that CPython's
implementation of \lstinline{math.isqrt} does not make any use of
floating-point.

Neither the Python language nor (prior to Python 3.12) the CPython
reference implementation of that language, provides a guarantee of either IEEE
754 format floating-point or correct rounding of floating-point arithmetic
operations. As such, any floating-point-based method for direct computation of
an integer square root would need a correctness check along with a pure-integer
fallback method for the case where the floating-point square root operation
fails to provide sufficient accuracy. For this reason, use of floating-point
was avoided in the implementation of \lstinline{math.isqrt}.

Despite the lack of language guarantees, use of IEEE 754 floating-point is
almost ubiquitous on platforms that support Python, and on almost all systems
currently in use Python's \lstinline{float} type maps to the IEEE 754 ``double
precision'' binary64 interchange format, defined in section~3.6 of IEEE
754-2019. Moreover, floating-point square root operations implemented in
hardware are invariably correctly rounded. From this point onwards, we assume
that Python's \lstinline{float} type \emph{does} use the binary64 format, and
that basic arithmetic operations and the standard library \lstinline{math.sqrt}
function are all correctly rounded, using the default IEEE 754
``roundTiesToEven" rounding mode.

First some definitions: given a real number~$x$, write $\float(x)$ for the
nearest (under ``roundTiesToEven") binary64 floating-point number to~$x$. If
$x$ has magnitude $2^{1024} - 2^{970}$ or greater, $\float(x)$ is the
appropriately-signed infinity. If $x$ is already a finite floating-point
number, we implicitly regard it as a real number when necessary. Given a finite
nonnegative binary64 floating-point number $x$, write $\fsqrt(x)$ for the
correctly-rounded square root of~$x$, and $\fsqr(x)$ for the correctly-rounded
square of $x$. In other words, $\fsqrt(x) = \float(\sqrt x)$ and $\fsqr(x) =
\float(x^2)$.

We first prove that if $n$ is not too large then $\floor{\fsqrt(n)}$ gives the
integer square root of~$n$.

\begin{lemma}
  Suppose that $n$ is a nonnegative integer satisfying $n < 2^{52}$. Then
  $\floor{\fsqrt(n)}$ is the integer square root of~$n$.
\end{lemma}

\begin{proof}
  The lemma is clearly true for~$n=0$, so for the remainder of the proof
  we assume that~$n$ is positive.

  Write $a$ for the integer square root of $n$. Then $a^2 \le n < (a + 1)^2$,
  so $a \le \sqrt n < a + 1$, and since both $a$ and $a + 1$ are small
  enough to be exactly representable as floats, and the $\float$ operation
  is monotonic, it follows that
  $$a = \float(a) \le \float(\sqrt n) \le \float(a + 1) = a + 1.$$
  So
  $$a \le \fsqrt(n) \le a + 1,$$
  and to prove the lemma, it suffices to eliminate
  the possibility that~$\float(\sqrt n) = a + 1$.

  Since~$n$ is positive, so is~$a$, and we can find an integer $k$ such that
  $2^k \le a < a + 1 \le 2^{k + 1}$. Any two consecutive floating-point numbers
  in the interval $[2^k, 2^{k+1}]$ are separated by exactly $2^{k-52}$, so the
  largest floating-point number that's strictly smaller than $a + 1$ is $a + 1
  - 2^{k - 52}.$ To avoid the possibility that $\sqrt n$ rounds up to $a + 1$,
  it's enough to show that $\sqrt n$ is closer to $a + 1 - 2^{k-52}$ than to~$a
  + 1$; that is, that
  $$\sqrt n < a + 1 - 2^{k-53}$$
  or equivalently that
  $$a + 1 - \sqrt n > 2^{k - 53}.$$
  But we have
  $$a + 1 - \sqrt n = \frac{(a + 1)^2 - n}{a + 1 + \sqrt n}.$$
  The numerator on the right-hand side above is at least~$1$, and since $\sqrt
  n < a + 1$, the denominator is less than $2(a+1)$, which is in turn less than
  or equal to $2^{k+2}$. So
  $$a + 1 - \sqrt n > 2^{-k-2}.$$ So we only need to show that $2^{-k-2} \ge
  2^{k-53}$. But from our assumptions, $a^2 \le n < 2^{52}$, so $a < 2^{26}$,
  so $k$ is at most~$25$ and if follows that $2^{-k-2} \ge 2^{-27} > 2^{-28}
  \ge 2^{k-53}$, as required.
\end{proof}

The upper bound can be extended slightly:

\begin{corollary}
  Suppose that~$n$ is a nonnegative integer satisfying $n < 2^{52} + 2^{27}$.
  Then $\floor{\fsqrt(n)}$ is the integer square root of~$n$. However, for~$n =
  2^{52} + 2^{27} = (2^{26} + 1)^2 - 1$, $\floor{\fsqrt(n)}$ gives~$2^{26} +
  1$, while the integer square root of~$n$ is $2^{26}$.
\end{corollary}

\begin{proof}
  We already proved the statement for $n < 2^{52}$. For any~$n$ in the range
  $2^{52} \le n < 2^{52} + 2^{27}$ the correct integer square root is $2^{26}$
  and it's enough to observe that the operation $\floor{\fsqrt(n)}$ is
  monotonic in~$n$ and that it gives the correct answer for both~$2^{52}$ and
  for~$2^{52} + 2^{27}$. The last statement follows by direct computation.
\end{proof}

Next we show that if all we need is a near square root of~$n$ (as defined in
section~\ref{new_method}), then we can use a much larger upper bound on~$n$.
First, a lemma leading to a well-known theorem: that the floating-point
square root operation recovers a float~$x$ from its (floating-point) square.

\begin{lemma}\label{float_scale}
  Suppose that $x$ and $y$ are positive real numbers such that $x = 2^e y$ for
  some integer $e$, and that both $x$ and $y$ lie in the half-open interval
  $[2^{-1022} - 2^{-1076}, 2^{1024} - 2^{970})$. Then $\float(x) = 2^e
  \float(y)$.
\end{lemma}

\begin{proof}
  The bounds on $x$ and~$y$ ensure that $\float(x) = 2^f \rint(x / 2^f)$, where
  $f = \floor{\log_2 x} - 52$, and similarly for~$y$. Here $\rint(x)$ is the
  operation that rounds a real number to the nearest integer, rounding halfway
  cases to the even integer. The result follows directly from this.
\end{proof}

The following fact is well known, but we state and prove it here for
convenience.

\begin{theorem}\label{sqrt_of_sqr}
  Suppose $x$ is an IEEE 754 binary64 floating-point number satisfying
  $2^{-511} \le x < 2^{512}$. Then $\fsqrt(\fsqr(x)) = x$.
\end{theorem}

\begin{proof}
  We first use the previous lemma to reduce to the case where $1/2 < x \le 1$.
  Choose an integer $e$ and floating-point number $y$ such that $1/2 < y \le 1$
  and
  $$x = 2^e y.$$
  Then
  $$x^2 = 2^{2e}y^2$$
  so applying Lemma~\ref{float_scale},
  $$\float(x^2) = 2^{2e}\float(y^2).$$
  By definition of $\fsqr$, this can be rewritten as
  $$\fsqr(x) = 2^{2e}\fsqr(y).$$
  Now taking square roots of both sides,
  $$\sqrt{\fsqr(x)} = 2^e \sqrt{\fsqr(y)}.$$
  Applying Lemma~\ref{float_scale} again,
  $$\float(\sqrt{\fsqr(x)}) = 2^e \float(\sqrt{\fsqr(y)}),$$
  or in other words
  $$\fsqrt(\fsqr(x)) = 2^e \fsqrt(\fsqr(y)).$$
  So to prove that $\fsqrt(\fsqr(x)) = x$, it's enough to prove that
  $\fsqrt(\fsqr(y)) = y$, since we then have
  $$\fsqrt(\fsqr(x)) = 2^e \fsqrt(\fsqr(y)) = 2^e y = x.$$
  So from this point on, we assume that $1/2 < x \le 1$.

  Let $z = \fsqr(x)$ be the closest float to $x^2$. Then $1/4 \le z \le 1$ and
  since floats are spaced at most $2^{-53}$ apart within the interval $[1/4,
  1]$,
  $$\abs{z - x^2} \le 2^{-54}.$$ Now we have
  $$\sqrt z - x = \frac{(\sqrt z - x)(\sqrt z + x)}{\sqrt z + x} = \frac{z -
  x^2}{\sqrt z + x}$$ and since $1/2 < x$ and $1/2 \le \sqrt z $, $\sqrt z + x
  > 1$. It follows that
  $$\abs{\sqrt z - x} < 2^{-54}$$ and so since floats in the interval $[1/2,
  1]$ are spaced exactly $2^{-53}$ apart, it follows that $x$ is the unique
  closest float to $\sqrt z$, so $x = \fsqrt(z)$ as required.
\end{proof}

The next corollary follows immediately from Theorem~\ref{sqrt_of_sqr}, together
with the fact that any nonnegative integer not exceeding $2^{53}$ can be
exactly represented in binary64 floating-point.

\begin{corollary}
  Suppose $n$ is a nonnegative integer satisfying $n \le 2^{53}$. Then
  $\fsqrt(\fsqr(n)) = n$.
\end{corollary}

Now we're in a position to prove that for sufficiently small~$n$, the
floating-point square root can be used to compute a near square root of~$n$.

\begin{corollary}
  Suppose $n$ is a positive integer satisfying $n \le 2^{106}$. Then
  $\floor{\fsqrt(\rnd(n))}$ is a near square root of $n$.
\end{corollary}

\begin{proof}
  First suppose that $n$ is a perfect square: $n = a^2$. Then $\rnd(n) =
  \fsqr(a)$, so from the previous corollary, $\fsqrt(\rnd(n)) =
  \fsqrt(\fsqr(a)) = a$.

  Now suppose that $n$ is not a perfect square, so that $a^2 < n < (a+1)^2$
  for some nonnegative integer $a < 2^{53}$. Then $\rnd$ is monotonic, so
    $$\rnd(a^2) \le \rnd(n) \le \rnd((a+1)^2)$$
  or equivalently,
    $$\fsqr(a) \le \rnd(n) \le \fsqr(a+1).$$
  Similarly, $\fsqrt$ is monotonic, so applying $\fsqrt$ throughout
  and using the previous corollary,
    $$a \le \fsqrt(\rnd(n)) \le a + 1.$$
  Hence $\floor{\fsqrt(\rnd n)}$ is either $a$ or $a+1$, so $a$ is a near
  square root of $n$.
\end{proof}

In fact, $\floor{\fsqrt(\rnd n)}$ gives a near square root of~$n$ for all $n
\le 2^{106} + 2^{54}$. The smallest~$n$ for which it fails to give a near
square root is $n = 2^{106} + 2^{54} + 1 = (2^{53} + 1)^2$: for this~$n$,
the only possible near square root is~$2^{53 + 1}$, which is not representable
in the IEEE 754 binary64 floating-point format.

Listing~\ref{isqrt_accelerated} shows a version of the iterative
algorithm in Listing~\ref{isqrt_iterative}, modified to use floating-point
arithmetic to compute the initial value of~$a$.  If~$n < 2^{106}$,
$a$ is already a near square root of~$n$ and the \lstinline$while$ loop is
executed zero times. More generally, the number of iterations required
is~$\floor{\log_2((\log_2n)/53)}$ for~$n \ge 2^{53}$, and~$0$ otherwise.

\lstinputlisting[
  label=isqrt_accelerated,
  float,
  frame=single,
  caption={Integer square root using floating-point quick start}]
  {isqrt_accelerated.py}

% \section{To do}

% - Running time analysis, and real-world timings to back it up. Asymptotically,
% isqrt of a $2n$-bit integer should be faster than division of a $2n$-bit integer
% by an $n$-bit dividend.
% - Variants: ceiling of square root, nearest integer to square root.
% - Reference(s) for "Many number-theoretic and cryptographic algorithms ..." Cohen?
% - Reference for Java BigInteger.sqrt
% - Reference for Heron's method / Babylonian method / Newton's method.
% - Reference for assertion that the number of correct decimal places
% doubles with each iteration.
% - Add comparison with GMP's recursive square root algorithm.

% Complexity analysis: write S(n) as an upper bound for the cost of squaring an
% n-bit number, and D(n) as an upper bound for the cost of dividing a 2n-bit
% number by an n-bit number.  We can reasonably assume that S(n/2) <= S(n)/2
% and D(n/2) <= D(n)/2, for n sufficiently large. All other operations involved
% in the algorithm are linear time in the number of bits of $n$.


\end{document}
